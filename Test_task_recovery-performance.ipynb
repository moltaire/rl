{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test parameter recovery performance for different tasks and task properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import rl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "matplotlib = rl.plot_utils.set_mpl_defaults(matplotlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of tasks considered\n",
    "\n",
    "## Reversal Learning Task (`rv`-Task)\n",
    "\n",
    "As a reference, this type of task is used in Kahnt, Park et al. (2008, Journal of Cognitive Neuroscience; \"KP08\").  \n",
    "In this task, only one state is visible to the agent (every trial looks identical to the agent).  \n",
    "A latent (unobservable) true state governs the reward probabilities of different actions. The latent state switches periodically, dividing the task into episodes.\n",
    "\n",
    "The properties of this task that can be adjusted are:\n",
    "\n",
    "1. The set of latent states that the task switches between. Each latent state in the state set contains the probabilities of different rewards associated the different actions.\n",
    "3. The transition properties between the latent states (i.e., the conditions and / or probabilities under which the latent state changes). This includes, for example, the minimum and maximum number of trials in each episode (without state switches).\n",
    "3. The number of trials and blocks.\n",
    "\n",
    "## Multiple State Task (`ms`-Task)\n",
    "\n",
    "This type of task was, for example, used in Lefebvre et al. (2017, Nature Human Behaviour; \"L17\").  \n",
    "Here, the states controlling the reward probabilities of different actions are observable by the agent (e.g., each state is associated with a different set of symbols that are presented in a trial).\n",
    "\n",
    "The properties of this task that can be adjusted are:\n",
    "1. The set of (observable) states, containing the probabilities of different rewards associated with different actions in each trial.\n",
    "2. The sequence and frequence of different states occuring.\n",
    "2. The number of trials and blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set of tasks to be compared\n",
    "\n",
    "In this analysis, I will consider the following task instances, for each varying the number of trials, and performing a parameter recovery analysis, to address the question, which task is best suited for recovering the learning rate parameters of a basic dual-learning-rate delta-rule agent:\n",
    "\n",
    "Each of the following tasks will have two possible actions (say, left and right).\n",
    "\n",
    "## `rv`-Task variants\n",
    "\n",
    "I test 6 variants of the `rv`-Task, obtained by combining three sets of states with two settings of transition \n",
    "\n",
    "- State sets\n",
    "    - A: 80:20, 20:80, 50:50 (as in KP08)\n",
    "        - `{\"Right correct\": {p_r=[0.2, 0.8], rewards=[0.5, 0]}}`\n",
    "        - `{\"Left correct\": {p_r=[0.8, 0.2], rewards=[0.5, 0]}}`\n",
    "        - `{\"Both correct\": {p_r=[0.5, 0.5], rewards=[0.5, 0]}}`\n",
    "    - B: 80:20, 20:80 (no 50:50)\n",
    "        - `{\"Right correct\": {p_r=[0.2, 0.8], rewards=[0.5, 0]}}`\n",
    "        - `{\"Left correct\": {p_r=[0.8, 0.2], rewards=[0.5, 0]}}`\n",
    "\n",
    "\n",
    "- Transition settings\n",
    "    - X: `min=10, max=16, acc=0.7` (as in KP08)\n",
    "    - Y: `min=5, max=20, acc=0.7`\n",
    "\n",
    "\n",
    "## `ms`-Task variants\n",
    "\n",
    "I test 3 variants of the `ms`-Task, obtained by combining three sets of states with two settings of transition \n",
    "\n",
    "- State sets\n",
    "    - C: Gains vs. Nothing (as in L17, Exp. 1)\n",
    "        - `{\"both bad\": {p_r=[0.25, 0.25], rewards=[0.5, 0]}}`\n",
    "        - `{\"left good\": {p_r=[0.75, 0.25], rewards=[0.5, 0]}}`\n",
    "        - `{\"right good\": {p_r=[0.25, 0.75], rewards=[0.5, 0]}}`\n",
    "        - `{\"both good\": {p_r=[0.75, 0.75], rewards=[0.5, 0]}}`\n",
    "    - D: Gains vs. Losses (as in L17, Exp. 2)\n",
    "        - `{\"both bad\": {p_r=[0.25, 0.25], rewards=[0.5, -0.5]}}`\n",
    "        - `{\"left good\": {p_r=[0.75, 0.25], rewards=[0.5, -0.5]}}`\n",
    "        - `{\"right good\": {p_r=[0.25, 0.75], rewards=[0.5, -0.5]}}`\n",
    "        - `{\"both good\": {p_r=[0.75, 0.75], rewards=[0.5, -0.5]}}`\n",
    "\n",
    "- Sequence, frequence\n",
    "    - Z: random sequence of states, equal frequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Structure of a single recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 1.1 Set up Reversal-Learning-Task (rv) from Kahnt, Park et al. (2008)\n",
    "# ------------------------------------------------------------------------\n",
    "print(\"1.1 Set up Reversal-Learning-Task (rv) from Kahnt, Park et al. (2008)\")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "\n",
    "# Set up the three different states (we changed the rewards from 1 to 0.5, for parameter values to better work with both tasks)\n",
    "rv_states = {\n",
    "    # 20:80\n",
    "    0: {\"p_r\": [0.2, 0.8], \"a_correct\": [1], \"rewards\": [0.5, 0]},\n",
    "    # 80:20\n",
    "    1: {\"p_r\": [0.8, 0.2], \"a_correct\": [0], \"rewards\": [0.5, 0]},\n",
    "    # 50:50\n",
    "    2: {\"p_r\": [0.5, 0.5], \"a_correct\": [0, 1], \"rewards\": [0.5, 0]},\n",
    "}\n",
    "rv_task_vars = rl.task.TaskVars(\n",
    "    n_trials=100,\n",
    "    n_blocks=5,\n",
    "    n_options=2,\n",
    "    states=rv_states,\n",
    "    n_trials_reversal_min=10,  # minimum number of trials before reversal\n",
    "    n_trials_reversal_max=16,  # maximum number of trials without reversal\n",
    "    p_correct_reversal_min=0.7,  # minimum accuracy before reversal before `n_trials_reversal_max`\n",
    ")\n",
    "\n",
    "rv_task = rl.task.ReversalLearningTask(task_vars=rv_task_vars)\n",
    "print(rv_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 2. Set up Dual-Learning-Rate Agent\n",
    "# -------------------------------------\n",
    "print(\"2. Set up Dual-Learning-Rate Agent\")\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "\n",
    "# Use mean parameters from Lefebvre et al. (2017)\n",
    "agent_vars = rl.agent.AgentVars(\n",
    "    alpha_pos=0.36, alpha_neg=0.22, beta=(1 / 0.13), variant=\"delta\"\n",
    ")\n",
    "agent = rl.agent.DualLearningRateAgent(\n",
    "    agent_vars=agent_vars, n_options=rv_task.task_vars.n_options\n",
    ")\n",
    "print(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to estimate\n",
    "parameters = [\"alpha_pos\", \"alpha_neg\", \"beta\"]\n",
    "\n",
    "# Set boundaries (used to initialize and constrain estimation)\n",
    "bounds = {\"alpha_pos\": (0, 1), \"alpha_neg\": (0, 1), \"beta\": (0, 20)}\n",
    "\n",
    "# Initialize estimation variables and estimation instance\n",
    "# Note, that we also need to specify the agent_class (i.e., the agent \"model\")\n",
    "est_vars = rl.estimation.EstimationVars(\n",
    "    rv_task_vars,\n",
    "    agent_class=rl.agent.DualLearningRateAgent,\n",
    "    parameters=parameters,\n",
    "    bounds=bounds,\n",
    "    n_sp=2,  # number of estimatino starting points\n",
    ")\n",
    "est = rl.estimation.Estimation(est_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 # number of individuals / set of parameters to recover\n",
    "\n",
    "parameter_values = dict(\n",
    "    alpha_pos=np.random.uniform(*bounds[\"alpha_pos\"], N),\n",
    "    alpha_neg=np.random.uniform(*bounds[\"alpha_neg\"], N),\n",
    "    beta=np.random.uniform(*bounds[\"beta\"], N),\n",
    ")\n",
    "\n",
    "result = rl.recovery.run_estimate_recovery(\n",
    "    task=rv_task, agent=agent, est=est, parameter_values=parameter_values\n",
    ")\n",
    "\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot recovery results\n",
    "fig, axs = plt.subplots(1, 3, figsize=(4.5, 1.5), dpi=200)\n",
    "for p, parameter in enumerate([\"alpha_pos\", \"alpha_neg\", \"beta\"]):\n",
    "    axs[p].set_title(parameter)\n",
    "    axs[p].plot(\n",
    "        result[parameter],\n",
    "        result[f\"{parameter}_hat\"],\n",
    "        \"o\",\n",
    "        markeredgewidth=0.5,\n",
    "        markersize=3,\n",
    "        clip_on=False,\n",
    "    )\n",
    "    axs[p].plot(\n",
    "        bounds[parameter], bounds[parameter], zorder=-1, linewidth=0.5, color=\"black\"\n",
    "    )\n",
    "    axs[p].set_xlabel(\"Generating\")\n",
    "    axs[p].set_ylabel(\"Recovered\")\n",
    "    axs[p].set_xlim(bounds[parameter])\n",
    "    axs[p].set_ylim(bounds[parameter])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run recovery for all task variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General settings\n",
    "np.random.seed(1)\n",
    "\n",
    "n_trials = 50  # numbers of trials per block\n",
    "n_sp = 5  # numbers of estimation starting points\n",
    "N = 100  # numbers of subjects with random (uniform within bounds) parameter values\n",
    "block_range = range(1, 7)  # range of number of blocks to be varied\n",
    "\n",
    "parameter_values = dict(\n",
    "    alpha_pos=np.random.uniform(*bounds[\"alpha_pos\"], N),\n",
    "    alpha_neg=np.random.uniform(*bounds[\"alpha_neg\"], N),\n",
    "    beta=np.random.uniform(*bounds[\"beta\"], N),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_state_sets = {\n",
    "    \"A\": {\n",
    "        0: {\"p_r\": [0.2, 0.8], \"a_correct\": [1], \"rewards\": [0.5, 0]},\n",
    "        1: {\"p_r\": [0.8, 0.2], \"a_correct\": [0], \"rewards\": [0.5, 0]},\n",
    "        2: {\"p_r\": [0.5, 0.5], \"a_correct\": [0, 1], \"rewards\": [0.5, 0]},\n",
    "    },\n",
    "    \"B\": {\n",
    "        0: {\"p_r\": [0.2, 0.8], \"a_correct\": [1], \"rewards\": [0.5, 0]},\n",
    "        1: {\"p_r\": [0.8, 0.2], \"a_correct\": [0], \"rewards\": [0.5, 0]},\n",
    "    },\n",
    "}\n",
    "\n",
    "rv_transition_sets = {\n",
    "    \"X\": {\n",
    "        \"n_trials_reversal_min\": 10,\n",
    "        \"n_trials_reversal_max\": 16,\n",
    "        \"p_correct_reversal_min\": 0.7,\n",
    "    },\n",
    "    \"Y\": {\n",
    "        \"n_trials_reversal_min\": 5,\n",
    "        \"n_trials_reversal_max\": 20,\n",
    "        \"p_correct_reversal_min\": 0.7,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rv = []\n",
    "\n",
    "pbar = tqdm(total=2 * len(rv_state_sets) * len(rv_transition_sets) * len(block_range))\n",
    "\n",
    "for variant in [\"r\", \"delta\"]:\n",
    "    for state_set_label, state_set in rv_state_sets.items():\n",
    "        for transition_set_label, transition_set in rv_transition_sets.items():\n",
    "            for n_blocks in block_range:\n",
    "\n",
    "                # Set up task\n",
    "                rv_task_vars = rl.task.TaskVars(\n",
    "                    n_trials=n_trials,\n",
    "                    n_blocks=n_blocks,\n",
    "                    n_options=2,\n",
    "                    states=state_set,\n",
    "                    n_trials_reversal_min=transition_set[\n",
    "                        \"n_trials_reversal_min\"\n",
    "                    ],  # minimum number of trials before reversal\n",
    "                    n_trials_reversal_max=transition_set[\n",
    "                        \"n_trials_reversal_max\"\n",
    "                    ],  # maximum number of trials without reversal\n",
    "                    p_correct_reversal_min=transition_set[\n",
    "                        \"p_correct_reversal_min\"\n",
    "                    ],  # minimum accuracy before reversal before `n_trials_reversal_max`\n",
    "                )\n",
    "                rv_task = rl.task.ReversalLearningTask(task_vars=rv_task_vars)\n",
    "\n",
    "                # Set up agent\n",
    "                # Use mean parameters from Lefebvre et al. (2017)\n",
    "                agent_vars = rl.agent.AgentVars(\n",
    "                    alpha_pos=0.36, alpha_neg=0.22, beta=(1 / 0.13), variant=variant\n",
    "                )\n",
    "                agent = rl.agent.DualLearningRateAgent(\n",
    "                    agent_vars=agent_vars, n_options=rv_task.task_vars.n_options\n",
    "                )\n",
    "\n",
    "                # Set up estimation\n",
    "                # Define parameters to estimate\n",
    "                parameters = [\"alpha_pos\", \"alpha_neg\", \"beta\"]\n",
    "\n",
    "                # Set boundaries (used to initialize and constrain estimation)\n",
    "                bounds = {\"alpha_pos\": (0, 1), \"alpha_neg\": (0, 1), \"beta\": (0, 20)}\n",
    "\n",
    "                # Initialize estimation variables and estimation instance\n",
    "                # Note, that we also need to specify the agent_class (i.e., the agent \"model\")\n",
    "                est_vars = rl.estimation.EstimationVars(\n",
    "                    rv_task_vars,\n",
    "                    agent_class=rl.agent.DualLearningRateAgent,\n",
    "                    parameters=parameters,\n",
    "                    bounds=bounds,\n",
    "                    n_sp=n_sp,\n",
    "                )\n",
    "                est = rl.estimation.Estimation(est_vars)\n",
    "\n",
    "                # Run recovery\n",
    "                result = rl.recovery.run_estimate_recovery(\n",
    "                    task=rv_task,\n",
    "                    agent=agent,\n",
    "                    est=est,\n",
    "                    parameter_values=parameter_values,\n",
    "                    verbose=False,\n",
    "                )\n",
    "                result[\"state_set\"] = state_set_label\n",
    "                result[\"transition_set\"] = transition_set_label\n",
    "                result[\"task\"] = \"rv\"\n",
    "                result[\"variant\"] = variant\n",
    "                results_rv.append(result)\n",
    "                pd.concat(results_rv).to_csv(\n",
    "                    \"test_task_recovery_rv.csv\"\n",
    "                )  # intermediate save\n",
    "                pbar.update(1)\n",
    "\n",
    "results_rv = pd.concat(results_rv)\n",
    "results_rv.to_csv(\"test_task_recovery_rv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_state_sets = {\n",
    "    \"D\": {\n",
    "        0: {\"p_r\": [0.25, 0.25], \"a_correct\": [0, 1], \"rewards\": [0.5, 0]},\n",
    "        1: {\"p_r\": [0.75, 0.25], \"a_correct\": [0], \"rewards\": [0.5, 0]},\n",
    "        2: {\"p_r\": [0.25, 0.75], \"a_correct\": [1], \"rewards\": [0.5, 0]},\n",
    "        3: {\"p_r\": [0.75, 0.75], \"a_correct\": [0, 1], \"rewards\": [0.5, 0]},\n",
    "    },\n",
    "    \"E\": {\n",
    "        0: {\"p_r\": [0.25, 0.25], \"a_correct\": [0, 1], \"rewards\": [0.5, -0.5]},\n",
    "        1: {\"p_r\": [0.75, 0.25], \"a_correct\": [0], \"rewards\": [0.5, -0.5]},\n",
    "        2: {\"p_r\": [0.25, 0.75], \"a_correct\": [1], \"rewards\": [0.5, -0.5]},\n",
    "        3: {\"p_r\": [0.75, 0.75], \"a_correct\": [0, 1], \"rewards\": [0.5, -0.5]},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ms = []\n",
    "\n",
    "pbar = tqdm(total=len(ms_state_sets) * len(block_range))\n",
    "\n",
    "for variant in [\"r\", \"delta\"]:\n",
    "    for state_set_label, state_set in ms_state_sets.items():\n",
    "        for n_blocks in block_range:\n",
    "\n",
    "            # Build sequence of trials / states (n_blocks x n_trials)\n",
    "            n_states = len(state_set)\n",
    "            state_sequence = np.repeat(\n",
    "                np.repeat(np.arange(n_states), n_trials // n_states)[:, None],\n",
    "                n_blocks,\n",
    "                axis=1,\n",
    "            )\n",
    "\n",
    "            ## Shuffle within each block\n",
    "            [\n",
    "                np.random.shuffle(state_sequence[:, block])\n",
    "                for block in range(state_sequence.shape[1])\n",
    "            ]\n",
    "\n",
    "            # Set up task\n",
    "            ms_task_vars = rl.task.TaskVars(\n",
    "                n_options=2, states=state_set, state_sequence=state_sequence.T\n",
    "            )\n",
    "            ms_task = rl.task.MultipleStateTask(task_vars=ms_task_vars)\n",
    "\n",
    "            # Set up agent\n",
    "            # Use mean parameters from Lefebvre et al. (2017)\n",
    "            agent_vars = rl.agent.AgentVars(\n",
    "                alpha_pos=0.36, alpha_neg=0.22, beta=(1 / 0.13), variant=variant,\n",
    "            )\n",
    "            agent = rl.agent.DualLearningRateAgent(\n",
    "                agent_vars=agent_vars,\n",
    "                n_options=ms_task.task_vars.n_options,\n",
    "                n_states=n_states,\n",
    "            )\n",
    "\n",
    "            # Set up estimation\n",
    "            # Define parameters to estimate\n",
    "            parameters = [\"alpha_pos\", \"alpha_neg\", \"beta\"]\n",
    "\n",
    "            # Set boundaries (used to initialize and constrain estimation)\n",
    "            bounds = {\"alpha_pos\": (0, 1), \"alpha_neg\": (0, 1), \"beta\": (0, 20)}\n",
    "\n",
    "            # Initialize estimation variables and estimation instance\n",
    "            # Note, that we also need to specify the agent_class (i.e., the agent \"model\")\n",
    "            est_vars = rl.estimation.EstimationVars(\n",
    "                ms_task_vars,\n",
    "                agent_class=rl.agent.DualLearningRateAgent,\n",
    "                parameters=parameters,\n",
    "                bounds=bounds,\n",
    "                n_sp=n_sp,\n",
    "            )\n",
    "            est = rl.estimation.Estimation(est_vars)\n",
    "\n",
    "            # Run recovery\n",
    "            result = rl.recovery.run_estimate_recovery(\n",
    "                task=ms_task,\n",
    "                agent=agent,\n",
    "                est=est,\n",
    "                parameter_values=parameter_values,\n",
    "                verbose=False,\n",
    "            )\n",
    "            result[\"state_set\"] = state_set_label\n",
    "            result[\"transition_set\"] = \"Z\"\n",
    "            result[\"task\"] = \"ms\"\n",
    "            result[\"variant\"] = variant\n",
    "            results_ms.append(result)\n",
    "            pd.concat(results_ms).to_csv(\n",
    "                \"test_task_recovery_ms.csv\"\n",
    "            )  # intermediate save\n",
    "            pbar.update(1)\n",
    "\n",
    "results_ms = pd.concat(results_ms)\n",
    "results_ms.to_csv(\"test_task_recovery_ms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([results_rv, results_ms]).reset_index(drop=True)\n",
    "results.to_csv(\"test_task_recovery.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse results\n",
    "\n",
    "- Which task has the lower recovery error given the same number of trials?\n",
    "- Which task properties are associated with a lower recovery error?\n",
    "- How many trials are needed for \"good\" recovery?\n",
    "\n",
    "## Quantifying recovery performance\n",
    "\n",
    "I'll quantify error performance as the mean squared error between generating and recovered estimates, summed across the three parameters. Because the parameters have different scales (alphas go from 0 to 1, beta goes from 0 to 20), I will weight errors for the beta parameter by $\\frac{1}{20}$\n",
    "\n",
    "$$e = \\sum_i{(\\hat{\\alpha}^+_i - \\alpha^+_i)^2 + (\\hat{\\alpha}^-_i - \\alpha^-_i)^2 + \\frac{1}{20}(\\hat{\\beta}_i - \\beta_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute error for each individual i\n",
    "for parameter in [\"alpha_pos\", \"alpha_neg\", \"beta\"]:\n",
    "    results[f\"error_{parameter}\"] = (\n",
    "        results[f\"{parameter}_hat\"] - results[parameter]\n",
    "    ) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"total_weighted_error\"] = (\n",
    "    results[\"error_alpha_pos\"]\n",
    "    + results[\"error_alpha_neg\"]\n",
    "    + (1 / 20) * results[\"error_beta\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby([\"task\", \"state_set\", \"transition_set\", \"n_blocks\"])[\n",
    "    [\"error_alpha_pos\", \"error_alpha_neg\", \"error_beta\", \"total_weighted_error\"]\n",
    "].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_errors = pd.melt(\n",
    "    results,\n",
    "    id_vars=[\"task\", \"state_set\", \"transition_set\", \"n_blocks\", \"idx\"],\n",
    "    value_vars=[\"error_alpha_pos\", \"error_alpha_neg\"],\n",
    "    var_name=\"parameter\",\n",
    "    value_name=\"error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    x=\"n_blocks\",\n",
    "    y=\"error\",\n",
    "    col=\"parameter\",\n",
    "    hue=\"task\",\n",
    "    data=alpha_errors,\n",
    "    kind=\"line\",\n",
    "    height=2,\n",
    "    aspect=1,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
